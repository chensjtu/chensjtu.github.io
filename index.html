<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta name="google-site-verification" content="uNRPSY4LpYw8yZ3es5Vl_bHu_fM9fMUrNnI0iaXvXXE" /> <!-- Google Search Console -->
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Chen Yang SJTU (æ¨è¾°)</title>

  <meta name="author" content="Chen Yang">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="icon" type="image/png" href="images/sjtu.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick-theme.min.css">
  <link rel="stylesheet" href="global.css">
  <link href="https://fonts.googleapis.com/css2?family=Lora:wght@400;700&display=swap" rel="stylesheet">

  <script type="text/javascript" src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/slick-carousel/1.8.1/slick.min.js"></script>
  <script src="global.js"></script>

</head>

<!-- <style>
  body {
    font-family: 'Times New Roman', Times, serif; /* é¦–é€‰Times New Romanï¼Œå¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ°Timesæˆ–é»˜è®¤è¡¬çº¿å­—ä½“ */
  }
</style> -->

<!-- <style>
  body {
    font-family: 'Calibri', Arial, sans-serif; /* é¦–é€‰Calibriï¼Œå¦‚æœæ²¡æœ‰åˆ™å›é€€åˆ°Arialæˆ–é»˜è®¤æ— è¡¬çº¿å­—ä½“ */
  }
</style> -->

<style>
/* åŸºç¡€å“åº”å¼æ ·å¼ */
@media screen and (max-width: 768px) {
table, tbody, tr, td {
  display: block;
  width: 100% !important;
  box-sizing: border-box;
}

/* æ”¹è¿›çš„åˆ—è¡¨æ ·å¼ */
ul {
  padding-left: 20px;
  margin-right: 10px; /* é˜²æ­¢æ–‡å­—è´´è¿‘å³è¾¹ç¼˜ */
}

li {
  width: 100%;
  word-wrap: break-word;      /* å…è®¸é•¿å•è¯æ¢è¡Œ */
  overflow-wrap: break-word;  /* ç°ä»£æµè§ˆå™¨ä½¿ç”¨ overflow-wrap */
  hyphens: auto;              /* åœ¨é€‚å½“çš„ä½ç½®æ·»åŠ è¿å­—ç¬¦ */
  white-space: normal;        /* ç¡®ä¿æ–‡æœ¬ä¼šæ¢è¡Œ */
}

/* ç¡®ä¿æ‰€æœ‰æ–‡æœ¬å†…å®¹éƒ½èƒ½æ¢è¡Œ */
td, p, heading {
  word-wrap: break-word;
  overflow-wrap: break-word;
  white-space: normal;
}
}
</style>

<style>
body {
  font-family: 'Lora', 'Times New Roman', serif; /* Fallback to generic serif if Lora is not available */
}
</style>

<style>
  .floating-card {
    display: flex; /* ä½¿ç”¨Flexboxè¿›è¡Œå¸ƒå±€ */
    width: 100%; /* å¡ç‰‡å®½åº¦è®¾ä¸º100% */
    border: 1px solid #ccc; /* å¯é€‰ï¼šä¸ºå¡ç‰‡æ·»åŠ è¾¹æ¡† */
    padding: 10px; /* å¯é€‰ï¼šä¸ºå¡ç‰‡æ·»åŠ å†…è¾¹è· */
    align-items: center; /* ç¡®ä¿å¡ç‰‡å†…çš„å…ƒç´ åœ¨å‚ç›´æ–¹å‘ä¸Šå±…ä¸­ */
  }

  .video-container {
    width: 30%; /* è§†é¢‘å®¹å™¨å®½åº¦è®¾ç½®ä¸º30% */
    display: flex; /* ä½¿ç”¨Flexboxå¯¹è§†é¢‘è¿›è¡Œå¸ƒå±€ */
    align-items: center; /* å‚ç›´å±…ä¸­è§†é¢‘ */
    justify-content: center; /* æ°´å¹³å±…ä¸­è§†é¢‘ */
    height: 100%; /* è®¾å®šé«˜åº¦ä¸º100%ç¡®ä¿è¶³å¤Ÿç©ºé—´å‚ç›´å±…ä¸­ */
  }

  .image-container {
    width: 30%;
    display: flex;
    align-items: center;
    justify-content: center;
    height: 100%;
  }

  .image-container img {
    width: 100%;
    height: auto;
    object-fit: cover; /* å¦‚æœå›¾åƒéœ€è¦è¦†ç›–æ•´ä¸ªåŒºåŸŸè€Œä¸ç•™ç©ºç™½ */
  }

  .paper-info {
    width: 70%; /* å‰©ä½™å†…å®¹å®½åº¦è®¾ç½®ä¸º70% */
    padding-left: 10px; /* å¯é€‰ï¼šä¸ºæ–‡æœ¬å†…å®¹æ·»åŠ ä¸€äº›å·¦è¾¹è· */
  }

  video {
    width: 100%; /* è§†é¢‘å®½åº¦ä¸º100% */
    height: auto; /* é«˜åº¦è‡ªåŠ¨ï¼Œä¿æŒè§†é¢‘çš„çºµæ¨ªæ¯” */
  }

  papertitle {
    display: block; /* è®©papertitleç‹¬å ä¸€è¡Œ */
    font-size: 1.0em; /* è®¾ç½®æ ‡é¢˜å¤§å° */
    font-weight: bold; /* è®¾ç½®æ ‡é¢˜ä¸ºç²—ä½“ */
  }
</style>

<body>
  <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto;">
    <tbody>
        <!-- Title and About section with image on the right -->
        <tr style="padding:0px">
            <td style="padding:2.5% 2.5% 0% 2.5%; width:65%; vertical-align:middle">
                <p style="text-align:center">
                    <h1>Chen Yang</h1>
                </p>
                <p><strong>About:</strong><br>
                    I am a 4th-year PhD student in Computer Science and Technology at the <a href="https://www.sjtu.edu.cn/">Shanghai Jiao Tong University(SJTU)</a>,
                    supervised by <a href="https://shenwei1231.github.io/">Prof. Wei Shen</a>. I also completed my bachelor's degree at SJTU in 2019.
                    Currently, I am a research intern at HUAWEI, working under the guidance of <a href="https://jaminfong.cn/">Dr. Jiemin Fang</a> and <a href="https://www.qitian1987.com/">Dr. Qi Tian</a>.
                    In 2023, I was honored to receive the <a href="https://miccai.org/index.php/about-miccai/awards/best-paper-award-and-young-scientist-award/">MICCAI Young Scientist Award</a>.
                </p>
            </td>
            <td style="padding:2.5%; width:35%; vertical-align:middle">
                <img style="width:75%; max-width:75%" alt="profile photo" src="images/siga2.png" class="hoverZoomLink">
            </td>
        </tr>
        <tr style="padding:0px">
            <td colspan="2" style="padding:0% 2.5% 2.5% 2.5%; width:100%; vertical-align:middle">
                <p><strong>Research Interests:</strong><br>
                    My research interests include CV&CG, particularly neural rendering and SLAM systems. I am dedicated to bridging these advanced technologies with practical applications in medical imaging and the general public.
                    I have proposed some thoughts about the future of neural rendering based 3D reconstruction in the following `Thoughts` section.
                    If you are interested in my research or have any questions, please feel free to contact me.
                </p>
                <p style="text-align:center">
                    <a href="mailto:ycyangchen@sjtu.edu.cn">Email</a> &nbsp/&nbsp
                    <a href="./cv.pdf">CV</a> &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=StdXTR8AAAAJ&hl">Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/chensjtu">Github</a> &nbsp/&nbsp
                    <a href="./proposal.pdf">Thoughts</a>
                </p>
            </td>
        </tr>
    </tbody>
  </table>  


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:5px;width:100%;vertical-align:middle">
          <heading>News</heading>
          <p> <strong>2025-02:</strong> ğŸš€ğŸš€ğŸš€ <strong><a href="https://gaussianobject.github.io/">GaussianObject</a></strong> receives over <strong><em>1000</em></strong> starsğŸŒŸ! </p>
          <p> <strong>2024-12:</strong> ğŸ’ªğŸ’ªğŸ’ª <strong><a href="https://jumpat.github.io/SAGA/">SAGA</a></strong> is finally accepted by <strong><em>AAAI 2025</em></strong>. </p>
          <p> <strong>2024-09:</strong> ğŸ¤—ğŸ¤—ğŸ¤— <strong><a href="https://gaussianobject.github.io/">GaussianObject</a></strong> is accepted by <strong><em>TOG(SIGGRAPH Asia 2024)</em></strong>. </p>
          <p> <strong>2024-06:</strong> ğŸ‰ğŸ‰ğŸ‰ <strong><a href="https://loping151.github.io/endogslam/">EndoGSLAM</a></strong> is accepted by <strong><em>MICCAI 2024</em></strong>. </p>
          <p> <strong>2024-04:</strong> ğŸ‰ğŸ‰ğŸ‰ <strong><a href="https://loping151.github.io/endogslam/">Forplane</a></strong> is accepted by <strong><em>TMI</em></strong>. </p>
          <p> <strong>2023-10:</strong> ğŸ¤—ğŸ¤—ğŸ¤— <strong><a href="https://loping151.github.io/endogslam/">Lerplane</a></strong> receives <strong><a href="https://miccai.org/index.php/about-miccai/awards/best-paper-award-and-young-scientist-award/">MICCAI Young Scientist Award 2023</a></strong>! </p>
        </td>
      </tr>
    </tbody>
</table>


  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:5px;width:100%;vertical-align:middle">
          <heading>Selected Publications</heading>
        </td>
      </tr>

      <tr>
        <td>
          <div class="floating-card">
            <div class="video-container">
              <video autoplay loop muted controls>
                <source src="videos/go.mov" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <div class="paper-info">
              <a href="https://gaussianobject.github.io/">
                <papertitle>GaussianObject: High-Quality 3D Object Reconstruction from Four Views with Gaussian Splatting</papertitle>
              </a>
              <strong><u>Chen Yang*</u></strong>, Sikuang Li*, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian
              <br>
              <strong><em>TOG(SIGGRAPH Asia)</em></strong>, 2024
              <br>
              <a href="https://gaussianobject.github.io/">Page</a> / <a href="https://arxiv.org/abs/2402.10259">arXiv</a> /
              <a href="https://github.com/GaussianObject/GaussianObject">Code</a> / <a href="https://asia.siggraph.org/2024/">ACM Paper</a> /
              <a href="https://www.youtube.com/watch?v=s5arAXdgdZQ&themeRefresh=1">Video</a> 
            </div>
          </div>
        </td>
      </tr>
    
      <tr>
        <td>
          <div class="floating-card">
            <div class="video-container">
              <video autoplay loop muted controls>
                <source src="https://loping151.github.io/endogslam/static/video/1_1.mp4" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <div class="paper-info">
              <a href="https://loping151.github.io/endogslam/">
                <papertitle>EndoGSLAM: Real-Time Dense Reconstruction and Tracking in Endoscopic Surgeries using Gaussian Splatting</papertitle>
              </a>
              Kailing Wang*, <strong><u>Chen Yang*</u></strong>, Yuehao Wang, Sikuang Li, Yan Wang, Qi Dou, Xiaokang Yang, Wei Shen
              <br>
              <strong><em>MICCAI</em></strong>, 2024
              <br>
              <a href="https://loping151.github.io/endogslam/">Page</a> / <a href="https://arxiv.org/abs/2403.15124">arXiv</a> /
              <a href="https://github.com/Loping151/EndoGSLAM">Code</a> / <a href="https://drive.google.com/drive/folders/1wT4cILcbf4TUlWlmK_wJPiIrZ2AqZ43W?usp=drive_link">Data</a>
            </div>
          </div>
        </td>
      </tr>

      <tr>
        <td>
          <div class="floating-card">
            <div class="paper-card-content">
              <div class="image-container">
                <video autoplay loop muted controls>
                  <source src="videos/sa3d.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
              </div>
              <div class="paper-info">
                <a href="https://jumpat.github.io/SA3D/">
                  <papertitle>Segment Anything in 3D with NeRFs</papertitle>
                </a>
                Jiazhong Cen*, Zanwei Zhou*, Jiemin Fang, <strong><u>Chen Yang</u></strong>, 
                Wei Shen, Lingxi Xie, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian
                <br>
                <strong><em>NeurIPS</em></strong>, 2023
                <br>
                <a href="https://jumpat.github.io/SA3D/">Page</a> / <a href="https://arxiv.org/abs/2304.12308">arXiv</a> /
                <a href="https://github.com/Jumpat/SegmentAnythingin3D">Code</a> 
              </div>
            </div>
          </div>
        </td>
      </tr>

      <tr>
        <td>
          <div class="floating-card">
            <div class="video-container">
              <video autoplay loop muted controls>
                <source src="videos/lerplane.mov" type="video/mp4">
                Your browser does not support the video tag.
              </video>
            </div>
            <div class="paper-info">
              <a href="https://link.springer.com/chapter/10.1007/978-3-031-43996-4_5">
                <papertitle>Lerplane: Neural Representations for Fast 4D Reconstruction of Deformable Tissues</papertitle>
              </a>
              <strong><u>Chen Yang</strong></u>, Kailing Wang, Yuehao Wang, Xiaokang Yang, Wei Shen
              <br>
              <strong><em>MICCAI</em></strong>, 2023, <strong>Oral</strong>, <strong>Young Scientist Award</strong>
              <br>
              <a href="https://link.springer.com/chapter/10.1007/978-3-031-43996-4_5">Paper</a> /
              <a href="https://github.com/Loping151/ForPlane">Code</a>
            </div>
          </div>
        </div>
      </td>
    </tr>

    <tr>
      <td>
        <div class="floating-card">
          <div class="video-container">
            <video autoplay loop muted controls>
              <source src="videos/nerfvs.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
          </div>
          <div class="paper-info">
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_NeRFVS_Neural_Radiance_Fields_for_Free_View_Synthesis_via_Geometry_CVPR_2023_paper.pdf">
              <papertitle>NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry Scaffolds</papertitle>
            </a>
            <strong><u>Chen Yang</strong></u>, Peihao Li, Zanwei Zhou, Shanxin Yuan, Bingbing Liu, Xiaokang Yang, Weichao Qiu, Wei Shen
            <br>
            <strong><em>CVPR</em></strong>, 2023
            <br>
            <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Yang_NeRFVS_Neural_Radiance_Fields_for_Free_View_Synthesis_via_Geometry_CVPR_2023_paper.pdf">Paper</a> /
            <a href="https://arxiv.org/abs/2304.06287">Code</a>
          </div>
        </div>
      </div>
    </td>
  </tr>

    </tbody>
  </table>

  <!-- Honers -->
  <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:5px;width:100%;vertical-align:middle">
          <heading>Honors and Awards</heading>
          <ul>
            <li><strong>2023:</strong> Intel Scholarship</li>
            <li><strong>2023:</strong> MICCAI Young Scientist Award</li>
            <li><strong>2022:</strong> 2nd Prize of National Post-Graduate Mathematical Contest</li>
            <li><strong>2021:</strong> National Scholarship</li>
            <li><strong>2021:</strong> 1st Prize of Huawei Chinese University ICT Competition</li>
          </ul>
        </td>
      </tr>
    </tbody>
  </table>

  <!-- Academic Service -->
  <table style="width:100%; border:0px; border-spacing:0px; border-collapse:separate; margin-right:auto; margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:5px;width:100%;vertical-align:middle">
          <heading>Academic Services</heading>
          <p><strong>Conference Reviewer:</strong></p>
          <li>CVPR '23, '24, '25'; ICCV '23, '25; NeurIPS '23, '24, '25; ECCV '24; AAAI '25; MICCAI '23, '24, '25</li>
          <p><strong>Journal Reviewer:</strong></p>
          <p>
            <li>TOG; TMI; TCSVT; TOMM</li>
          </p>
          <p><strong>Teaching Assistant:</strong></p>
          <ul>
            <li><strong>2019:</strong> MI 321: Course Design of Instrument Bus and Virtual Environment</li>
            <li><strong>2020:</strong> MI 318: Measuring and Controlling Circuit</li>
            <li><strong>2021:</strong> EE 334: Industrial Measurement and Control Technology and System</li>
          </ul>
        </td>
      </tr>
    </tbody>
  </table>

  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr>
        <td style="padding:5px;width:100%;vertical-align:middle">
          <heading>Publications</heading>
          <p> Full publications can also be found in my <a href="https://scholar.google.com/citations?user=StdXTR8AAAAJ&hl">Google Scholar</a>. </p>
  
          <h3>2025</h3>
          <p>
            Jiazhong Cen, Jiemin Fang, <strong><u>Chen Yang</u></strong>, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian. 
            Segment Any 3D Gaussians. 
            <strong><em>AAAI</em></strong>, 2025.
          </p>

          <h3>2024</h3>
          <p>
            <strong><u>Chen Yang*</u></strong>, Sikuang Li*, Jiemin Fang, Ruofan Liang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, Qi Tian. 
            GaussianObject: High-Quality 3D Object Reconstruction from Four Views with Gaussian Splatting. 
            <strong><em>ACM Trans. Graphics (Siggraph Asia)</em></strong>, 2024.
          </p>
  
          <p>
            <strong><u>Chen Yang*</u></strong>, Kailing Wang*, Yuehao Wang, Sikuang Li, Yan Wang, Qi Dou, Xiaokang Yang, Wei Shen. 
            EndoGSLAM: Real-time Dense Reconstruction and Tracking in Endoscopic Surgeries using Gaussian Splatting. 
            <strong><em>MICCAI</em></strong>, 2024.
          </p>
  
          <p>
            <strong><u>Chen Yang</u></strong>, Kailing Wang, Yuehao Wang, Qi Dou, Xiaokang Yang, Wei Shen. 
            Efficient Deformable Tissue Reconstruction via Orthogonal Neural Plane. 
            <strong><em>IEEE Trans. Medical Imaging (TMI)</em></strong>, 2024.
          </p>
  
          <p>
            Yichao Yan, Zanwei Zhou, Zi Wang, <strong><u>Chen Yang</u></strong>, Jingnan Gao, Xiaokang Yang. 
            DialogueNeRF: Towards Realistic Avatar Face-to-Face Conversation Video Generation. 
            <strong><em>Visual Intelligence</em></strong>, 2024.
          </p>

          <p>
            Kailing Wang*, <strong><u>Chen Yang*</u></strong>, Keyang Zhao, Xiaokang Yang, Wei Shen.
            Realistic surgical simulation from monocular videos.
            arXiv preprint, 2024.
          </p>

          <p>
            <strong><u>Chen Yang*</u></strong>, Haoyu Zhao*, Hao Wang, Xingyue Zhao, Wei Shen. 
            SG-GS: Photo-realistic Animatable Human Avatars with Semantically-Guided Gaussian Splatting. 
            arXiv preprint, 2024.
          </p>
  
          <p>
            <strong><u>Chen Yang*</u></strong>, Haoyu Zhao*, Hao Wang, Wei Shen. 
            CHASE: 3D-Consistent Human Avatars with Sparse Inputs via Gaussian Splatting and Contrastive Learning. 
            arXiv preprint, 2024.
          </p>
  
          <h3>2023</h3>
          <p>
            Jiazhong Cen*, Zanwei Zhou*, Jiemin Fang, <strong><u>Chen Yang</u></strong>, Wei Shen, Lingxi Xie, Dongsheng Jiang, Xiaopeng Zhang, Qi Tian. 
            Segment Anything in 3D with NeRFs. 
            <strong><em>NeurIPS</em></strong>, 2023.
          </p>
  
          <p>
            <strong><u>Chen Yang</u></strong>, Kailing Wang, Yuehao Wang, Xiaokang Yang, Wei Shen. 
            Neural Lerplane Representations for Fast 4D Reconstruction of Deformable Tissues. 
            <strong><em>MICCAI</em></strong>, 2023. <strong><em>(Oral, Young Scientist Award)</em></strong>
          </p>
  
          <p>
            <strong><u>Chen Yang</u></strong>, Peihao Li, Zanwei Zhou, Shanxin Yuan, Bingbing Liu, Xiaokang Yang, Weichao Qiu, Wei Shen. 
            NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry Scaffolds. 
            <strong><em>CVPR</em></strong>, 2023.
          </p>
  
          <p>
            Peihao Li, Shaohui Wang, <strong><u>Chen Yang</u></strong>, Bingbing Liu, Weichao Qiu, Haoqian Wang. 
            NeRF-MS: Neural Radiance Fields with Multi-Sequence. 
            <strong><em>ICCV</em></strong>, 2023.
          </p>
  
          <h3>2022</h3>
          <p>
            <strong><u>Chen Yang</u></strong>, Shun-Yu Yao, Zan-Wei Zhou, Bin Ji, Guang-Tao Zhai, Wei Shen. 
            Poxture: Human Posture Imitation Using Neural Texture. 
            <strong><em>IEEE Trans. Circuits and Systems for Video Technology (TCSVT)</em></strong>, 2022.
          </p>
  
          <p>
            Zanwei Zhou, Ruizhe Zhong, <strong><u>Chen Yang</u></strong>, Yan Wang, Xiaokang Yang, Wei Shen. 
            A k-variate Time Series is Worth k Words: Evolution of the Vanilla Transformer Architecture for Long-term Multivariate Time Series Forecasting. 
            arXiv preprint, 2022.
          </p>
  
          <p>
            Ruofan Liang, Jiahao Zhang, Haoda Li, <strong><u>Chen Yang</u></strong>, Yushi Guan, Nandita Vijaykumar. 
            SPIDR: SDF-based Neural Point Fields for Illumination and Deformation. 
            <strong><em>CVPR Workshop</em></strong>, 2022.
          </p>
  
          <h3>2021</h3>
          <p>
            Bin Ji, <strong><u>Chen Yang</u></strong>, Shunyu Yao, Ye Pan. 
            HPOF: 3D Human Pose Recovery from Monocular Video with Optical Flow. 
            <strong><em>ICMR</em></strong>, 2021.
          </p>
        </td>
      </tr>
    </tbody>
  </table>

</body>

</html>